# -*- coding: utf-8 -*-
"""llama-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gm-lwHAipcuEJLPyx-Q26qNY4e6LhUP9
"""

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

model= "/kaggle/input/llama-3/transformers/8b-chat-hf/1"

pipeline =transformers.pipeline(
    "text-generation",
    model=model,
    torch_dtype=torch.float16,
    device_map="auto",
)


def generate_answer(pipeline, question, temperature):
    try:
        prompt = f'''
        You are an AI content writer.
        Generate a well structured 100 words blog for the given topics below:
        Topics: {question}
        Answer:
        '''
        sequences = pipeline(
            prompt,
            do_sample=True,
            top_k=10,
            temperature=temperature,
            num_return_sequences=1,
            eos_token_id=pipeline.tokenizer.eos_token_id,
            max_length=256,
        )
        answer = sequences[0]['generated_text'][len(prompt):].strip()

        return answer

    except Exception as e:
        print("Error generating answer:", e)
        return None

question = "plastic pollution in our oceans"
answer = generate_answer(pipeline, question, temperature= 0.8)
if answer:
    print("Generated answer:", answer)
else:
    print("Failed to generate an answer.")

